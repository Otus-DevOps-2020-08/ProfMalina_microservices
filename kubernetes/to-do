#!/bin/sh

# yc vpc security-group create \
#   --name kubernetes-the-hard-way-allow-internal \
#   "--rule" "direction=egress,port=any,protocol=any,v4-cidrs=[10.240.0.0/24]" \
#   --network-name kubernetes-the-hard-way

# yc vpc security-group create \
#   --name kubernetes-the-hard-way-allow-external \
#   "--rule" "direction=ingress,port=22,protocol=tcp,v4-cidrs=[10.240.0.0/24,10.200.0.0/16]" \
#   "--rule" "direction=ingress,port=6443,protocol=tcp,v4-cidrs=[10.240.0.0/24,10.200.0.0/16]" \
#   "--rule" "direction=ingress,protocol=icmp,v4-cidrs=[10.240.0.0/24,10.200.0.0/16]" \
#   --network-name kubernetes-the-hard-way

# Installing the Client Tools

# Linux

wget -q --show-progress --https-only --timestamping https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/1.4.1/linux/cfssl https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/1.4.1/linux/cfssljson
chmod +x cfssl cfssljson
sudo mv cfssl cfssljson /usr/local/bin/

# Verification
cfssl version
cfssljson --version

# Install jq for parse yc
sudo apt-get install -y jq

# Install kubectl
wget https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/

# Verification
kubectl version --client

# Provisioning Compute Resources

# Virtual Private Cloud Network
yc vpc network create kubernetes-the-hard-way

# Create the kubernetes subnet in the kubernetes-the-hard-way VPC network
yc vpc subnet create --name kubernetes \
  --description "Subnet for kubernetes" \
  --folder-id b1gr89qnr9in6sgurknb \
  --network-name kubernetes-the-hard-way \
  --zone ru-central1-a \
  --range 10.240.0.0/24

# yc vpc subnet create --name pods \
#   --description "Pods subnet for kubernetes" \
#   --folder-id b1gr89qnr9in6sgurknb \
#   --network-name kubernetes-the-hard-way \
#   --zone ru-central1-a \
#   --range 10.200.0.0/16

# Firewall Rules

# TO-DO

# Kubernetes Public IP Address
yc vpc address create --name kubernetes-the-hard-way --external-ipv4 zone=ru-central1-b --folder-name infra

# Compute Instances

# Kubernetes Controllers
for i in 0 1 2; do
  yc compute instance create \
    --name controller-${i} \
    --hostname controller-${i} \
    --async \
    --memory=4 \
    --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts,size=50GB  \
    --network-interface subnet-name=kubernetes,address=10.240.0.1${i},nat-ip-version=ipv4 \
    --ssh-key ~/.ssh/appuser.pub
done

# Kubernetes Workers
for i in 0 1 2; do
  yc compute instance create \
    --name worker-${i} \
    --hostname worker-${i} \
    --async --memory=4 \
    --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-2004-lts,size=50GB \
    --network-interface subnet-name=kubernetes,address=10.240.0.2${i},nat-ip-version=ipv4 \
    --metadata pod-cidr=10.200.${i}.0/24 \
    --ssh-key ~/.ssh/appuser.pub
done


#===================================

# Provisioning a CA and Generating TLS Certificates
# Certificate Authority
cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
EOF

cat > ca-csr.json <<EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca

# Client and Server Certificates

cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:masters",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin

# The Kubelet Client Certificates

for instance in worker-0 worker-1 worker-2; do
cat > ${instance}-csr.json <<EOF
{
  "CN": "system:node:${instance}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

EXTERNAL_IP=$(yc compute instance get ${instance} --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')

INTERNAL_IP=$(yc compute instance get ${instance} --format json | jq '.network_interfaces[0].primary_v4_address.address' | tr -d '"')

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${instance},${EXTERNAL_IP},${INTERNAL_IP} \
  -profile=kubernetes \
  ${instance}-csr.json | cfssljson -bare ${instance}
done

# The Controller Manager Client Certificate

cat > kube-controller-manager-csr.json <<EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager

# The Kube Proxy Client Certificate

cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:node-proxier",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy

# The Scheduler Client Certificate

cat > kube-scheduler-csr.json <<EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-scheduler-csr.json | cfssljson -bare kube-scheduler

# The Kubernetes API Server Certificate

KUBERNETES_PUBLIC_ADDRESS=$(yc vpc address get kubernetes-the-hard-way --format json | jq '.external_ipv4_address.address' | tr -d '"')

KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local

cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=10.32.0.1,10.240.0.10,10.240.0.11,10.240.0.12,${KUBERNETES_PUBLIC_ADDRESS},127.0.0.1,${KUBERNETES_HOSTNAMES} \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes

# The Service Account Key Pair

cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  service-account-csr.json | cfssljson -bare service-account

# Distribute the Client and Server Certificates

# Copy the appropriate certificates and private keys to each worker instance:

for instance in worker-0 worker-1 worker-2; do
  EXTERNAL_IP=$(yc compute instance get ${instance} --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
  scp -i ~/.ssh/appuser -o StrictHostKeyChecking=no ca.pem ${instance}-key.pem ${instance}.pem yc-user@${EXTERNAL_IP}:~/
done

# Copy the appropriate certificates and private keys to each controller instance

for instance in controller-0 controller-1 controller-2; do
  EXTERNAL_IP=$(yc compute instance get ${instance} --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
  scp -i ~/.ssh/appuser -o StrictHostKeyChecking=no ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem yc-user@${EXTERNAL_IP}:~/
done

# Generating Kubernetes Configuration Files for Authentication

# Kubernetes Public IP Address
KUBERNETES_PUBLIC_ADDRESS=$(yc vpc address get kubernetes-the-hard-way --format json | jq '.external_ipv4_address.address' | tr -d '"')

# The kubelet Kubernetes Configuration File

# Generate a kubeconfig file for each worker node

for instance in worker-0 worker-1 worker-2; do
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-credentials system:node:${instance} \
    --client-certificate=${instance}.pem \
    --client-key=${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:node:${instance} \
    --kubeconfig=${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=${instance}.kubeconfig
done

# The kube-proxy Kubernetes Configuration File

kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials system:kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

# The kube-controller-manager Kubernetes Configuration File

kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig

# The kube-scheduler Kubernetes Configuration File

kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig

# The admin Kubernetes Configuration File

kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=admin.kubeconfig

kubectl config set-credentials admin \
    --client-certificate=admin.pem \
    --client-key=admin-key.pem \
    --embed-certs=true \
    --kubeconfig=admin.kubeconfig

kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=admin \
    --kubeconfig=admin.kubeconfig

kubectl config use-context default --kubeconfig=admin.kubeconfig

# Distribute the Kubernetes Configuration Files

for instance in worker-0 worker-1 worker-2; do
  EXTERNAL_IP=$(yc compute instance get ${instance} --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
  scp -i ~/.ssh/appuser -o StrictHostKeyChecking=no ${instance}.kubeconfig kube-proxy.kubeconfig yc-user@${EXTERNAL_IP}:~/
done

for instance in controller-0 controller-1 controller-2; do
  EXTERNAL_IP=$(yc compute instance get ${instance} --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
  scp -i ~/.ssh/appuser -o StrictHostKeyChecking=no admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig yc-user@${EXTERNAL_IP}:~/
done

# Generating the Data Encryption Config and Key

# The Encryption Key

ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

# The Encryption Config File

cat > encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF

# Copy the encryption-config.yaml encryption config file to each controller instance

for instance in controller-0 controller-1 controller-2; do
  EXTERNAL_IP=$(yc compute instance get ${instance} --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
  scp -i ~/.ssh/appuser -o StrictHostKeyChecking=no encryption-config.yaml yc-user@${EXTERNAL_IP}:~/
done

#WARNING!!!!!!!!!!!

ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get controller-0 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get controller-1 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get controller-2 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')


# Bootstrapping the etcd Cluster

# Bootstrapping an etcd Cluster Member

# Download the official etcd release binaries from the etcd GitHub project
wget -q --show-progress --https-only --timestamping "https://github.com/etcd-io/etcd/releases/download/v3.4.10/etcd-v3.4.10-linux-amd64.tar.gz"

# Extract and install the etcd server and the etcdctl command line utility
tar -xvf etcd-v3.4.10-linux-amd64.tar.gz
sudo mv etcd-v3.4.10-linux-amd64/etcd* /usr/local/bin/

# Configure the etcd Server
sudo mkdir -p /etc/etcd /var/lib/etcd
sudo chmod 700 /var/lib/etcd
sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/

# Create the etcd.service systemd unit file
INTERNAL_IP=$(curl -s -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip)
ETCD_NAME=$(hostname -s)

cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${INTERNAL_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Start the etcd Server

sudo systemctl daemon-reload
sudo systemctl enable etcd
sudo systemctl start etcd

# Verification
sudo ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.pem \
  --cert=/etc/etcd/kubernetes.pem \
  --key=/etc/etcd/kubernetes-key.pem


=========================

ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get controller-0 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get controller-1 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get controller-2 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')

# Bootstrapping the Kubernetes Control Plane

# Provision the Kubernetes Control Plane
sudo mkdir -p /etc/kubernetes/config

# Download and Install the Kubernetes Controller Binaries
wget -q --show-progress --https-only --timestamping \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kube-apiserver" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kube-controller-manager" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kube-scheduler" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kubectl"

# Install the Kubernetes binaries
chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/

# Configure the Kubernetes API Server
sudo mkdir -p /var/lib/kubernetes/

sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem encryption-config.yaml /var/lib/kubernetes/

# Create the kube-apiserver.service systemd unit file
INTERNAL_IP=$(curl -s -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip)

cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379,https://10.240.0.12:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
  --kubelet-https=true \\
  --runtime-config='api/all=true' \\
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Configure the Kubernetes Controller Manager
# Move the kube-controller-manager kubeconfig into place
sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/

# Create the kube-controller-manager.service systemd unit file
cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --bind-address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/var/lib/kubernetes/ca.pem \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Configure the Kubernetes Scheduler
# Move the kube-scheduler kubeconfig into place
sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/

# Create the kube-scheduler.yaml configuration file
cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF

# Create the kube-scheduler.service systemd unit file
cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Start the Controller Services
sudo systemctl daemon-reload
sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler

# Enable HTTP Health Checks
# Install a basic web server to handle HTTP health checks
sudo apt-get update
sudo apt-get install -y nginx

cat > kubernetes.default.svc.cluster.local <<EOF
server {
  listen      80;
  server_name kubernetes.default.svc.cluster.local;

  location /healthz {
     proxy_pass                    https://127.0.0.1:6443/healthz;
     proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem;
  }
}
EOF

sudo mv kubernetes.default.svc.cluster.local /etc/nginx/sites-available/kubernetes.default.svc.cluster.local

sudo ln -s /etc/nginx/sites-available/kubernetes.default.svc.cluster.local /etc/nginx/sites-enabled/

sudo systemctl restart nginx
sudo systemctl enable nginx

# Verification
kubectl get componentstatuses --kubeconfig admin.kubeconfig

# Test the nginx HTTP health check proxy
curl -H "Host: kubernetes.default.svc.cluster.local" -i http://127.0.0.1/healthz

===========================================

ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get controller-0 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')

# Запускается на одном, слушается весь кластер
# RBAC for Kubelet Authorization

cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF

cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF

========================================
The Kubernetes Frontend Load Balancer

# The Kubernetes Frontend Load Balancer

KUBERNETES_PUBLIC_ADDRESS=$(yc vpc address get kubernetes-the-hard-way --format json | jq '.external_ipv4_address.address' | tr -d '"')

CONTROLLER_0=$(yc compute instance get controller-0 --format json | jq '.network_interfaces[0].primary_v4_address.address' | tr -d '"')
CONTROLLER_1=$(yc compute instance get controller-1 --format json | jq '.network_interfaces[0].primary_v4_address.address' | tr -d '"')
CONTROLLER_2=$(yc compute instance get controller-2 --format json | jq '.network_interfaces[0].primary_v4_address.address' | tr -d '"')
SUBNET_ID=$(yc vpc subnet get kubernetes --format json | jq '.id' | tr -d '"')
yc load-balancer target-group create \
  --region-id ru-central1 \
  --name kubernetes-target-pool \
  --target subnet-id=${SUBNET_ID},address=${CONTROLLER_0} \
  --target subnet-id=${SUBNET_ID},address=${CONTROLLER_1} \
  --target subnet-id=${SUBNET_ID},address=${CONTROLLER_2}

yc load-balancer network-load-balancer create \
  --region-id ru-central1 \
  --name kubernetes-forwarding-rule \
  --target-group target-group-id=$(yc load-balancer target-group get kubernetes-target-pool --format json | jq '.id' | tr -d '"'),healthcheck-tcp-port=80,healthcheck-name=kubernetes \
  --listener name=kubernetes-target-pool,external-ip-version=ipv4,port=6443,external-address=${KUBERNETES_PUBLIC_ADDRESS}

curl --cacert ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version

=========================================

# Bootstrapping the Kubernetes Worker Nodes
ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get worker-0 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get worker-1 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get worker-2 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')

# Provisioning a Kubernetes Worker Node
sudo apt-get update
sudo apt-get -y install socat conntrack ipset

# Disable Swap
sudo swapon --show
sudo swapoff -a

# Download and Install Worker Binaries
wget -q --show-progress --https-only --timestamping \
  https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.18.0/crictl-v1.18.0-linux-amd64.tar.gz \
  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc91/runc.amd64 \
  https://github.com/containernetworking/plugins/releases/download/v0.8.6/cni-plugins-linux-amd64-v0.8.6.tgz \
  https://github.com/containerd/containerd/releases/download/v1.3.6/containerd-1.3.6-linux-amd64.tar.gz \
  https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.18.6/bin/linux/amd64/kubelet

# Create the installation directories

sudo mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes

# Install the worker binaries

mkdir containerd
tar -xvf crictl-v1.18.0-linux-amd64.tar.gz
tar -xvf containerd-1.3.6-linux-amd64.tar.gz -C containerd
sudo tar -xvf cni-plugins-linux-amd64-v0.8.6.tgz -C /opt/cni/bin/
sudo mv runc.amd64 runc
chmod +x crictl kubectl kube-proxy kubelet runc
sudo mv crictl kubectl kube-proxy kubelet runc /usr/local/bin/
sudo mv containerd/bin/* /bin/

# Configure CNI Networking
# Retrieve the Pod CIDR range for the current compute instance

POD_CIDR=$(curl -s -H "Metadata-Flavor: Google" \
  http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr)

# Create the bridge network configuration file
cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
{
    "cniVersion": "0.3.1",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "${POD_CIDR}"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
EOF

# Create the loopback network configuration file
cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.3.1",
    "name": "lo",
    "type": "loopback"
}
EOF

# Configure containerd
# Create the containerd configuration file
sudo mkdir -p /etc/containerd/

cat << EOF | sudo tee /etc/containerd/config.toml
[plugins]
  [plugins.cri.containerd]
    snapshotter = "overlayfs"
    [plugins.cri.containerd.default_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runc"
      runtime_root = ""
EOF

# Create the containerd.service systemd unit file

cat <<EOF | sudo tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
EOF

# Configure the Kubelet
sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
sudo mv ca.pem /var/lib/kubernetes/

# Create the kubelet-config.yaml configuration file

cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "${POD_CIDR}"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
EOF

# Create the kubelet.service systemd unit file

cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Configure the Kubernetes Proxy
sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig

# Create the kube-proxy-config.yaml configuration file

cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.200.0.0/16"
EOF

# Create the kube-proxy.service systemd unit file

cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Start the Worker Services

sudo systemctl daemon-reload
sudo systemctl enable containerd kubelet kube-proxy
sudo systemctl start containerd kubelet kube-proxy

# Verification

ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get controller-0 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"') "kubectl get nodes --kubeconfig admin.kubeconfig"

# Configuring kubectl for Remote Access

# The Admin Kubernetes Configuration File

KUBERNETES_PUBLIC_ADDRESS=$(yc vpc address get kubernetes-the-hard-way --format json | jq '.external_ipv4_address.address' | tr -d '"')

kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443

kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem

kubectl config set-context kubernetes-the-hard-way \
  --cluster=kubernetes-the-hard-way \
  --user=admin

kubectl config use-context kubernetes-the-hard-way

# Verification

kubectl get componentstatuses

kubectl get nodes

# Provisioning Pod Network Routes

# The Routing Table

for instance in worker-0 worker-1 worker-2; do
  yc compute instance get ${instance} --format json | jq '.network_interfaces[0].primary_v4_address.address' | tr -d '"'
done

# Routes

for i in 0 1 2; do
  yc vpc route-table create \
    --name=kubernetes-route-10-200-${i}-0-24 \
    --network-name kubernetes-the-hard-way \
    --route next-hop=10.240.0.2${i},destination=10.200.${i}.0/24
done

# List the routes in the kubernetes-the-hard-way VPC network

yc vpc route-table list

# Deploying the DNS Cluster Add-on

kubectl apply -f https://storage.googleapis.com/kubernetes-the-hard-way/coredns-1.7.0.yaml

# List the pods created by the kube-dns deployment

kubectl get pods -l k8s-app=kube-dns -n kube-system

# Verification

kubectl run busybox --image=busybox:1.29 --command -- sleep 3600
kubectl get pods -l run=busybox

# Retrieve the full name of the busybox pod

POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath="{.items[0].metadata.name}")

# Execute a DNS lookup for the kubernetes service inside the busybox pod

kubectl exec -ti $POD_NAME -- nslookup kubernetes
# не работает

ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get worker-0 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')
ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get controller-0 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')


# for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done

# Smoke Test

# Data Encryption

kubectl create secret generic kubernetes-the-hard-way --from-literal="mykey=mydata"

# Print a hexdump of the kubernetes-the-hard-way secret stored in etcd

ssh -i ~/.ssh/appuser -o StrictHostKeyChecking=no yc-user@$(yc compute instance get controller-0 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"') \
  "sudo ETCDCTL_API=3 etcdctl get \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.pem \
  --cert=/etc/etcd/kubernetes.pem \
  --key=/etc/etcd/kubernetes-key.pem\
  /registry/secrets/default/kubernetes-the-hard-way | hexdump -C"

# Deployments

kubectl create deployment nginx --image=nginx

# List the pod created by the nginx deployment

kubectl get pods -l app=nginx

# Port Forwarding

POD_NAME=$(kubectl get pods -l app=nginx -o jsonpath="{.items[0].metadata.name}")

# Forward port 8080 on your local machine to port 80 of the nginx pod

kubectl port-forward $POD_NAME 8080:80

curl --head http://127.0.0.1:8080

# Logs

kubectl logs $POD_NAME

# Exec

kubectl exec -ti $POD_NAME -- nginx -v

# Services

kubectl expose deployment nginx --port 80 --type NodePort

# Retrieve the node port assigned to the nginx service

NODE_PORT=$(kubectl get svc nginx --output=jsonpath='{range .spec.ports[0]}{.nodePort}')

# Create a firewall rule that allows remote access to the nginx node port

# TO-DO

# Retrieve the external IP address of a worker instance

EXTERNAL_IP=$(yc compute instance get worker-0 --format json | jq '.network_interfaces[0].primary_v4_address.one_to_one_nat.address' | tr -d '"')

# Make an HTTP request using the external IP address and the nginx node port

curl -I http://${EXTERNAL_IP}:${NODE_PORT}

# Cleaning Up

# Compute Instances
yc compute instance delete controller-0 controller-1 controller-2 worker-0 worker-1 worker-2

# Networking
yc load-balancer network-load-balancer delete kubernetes-forwarding-rule
yc load-balancer target-group delete kubernetes-target-pool
yc vpc address delete kubernetes-the-hard-way

for i in 0 1 2; do
  yc vpc route-table delete kubernetes-route-10-200-${i}-0-24
done

yc vpc subnet delete kubernetes

yc vpc networks delete kubernetes-the-hard-way
